<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RAP-MLLM</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Retrieval-Augmented Personalization for Multimodal Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://hoar012.github.io/" target="_blank">Haoran Hao</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://csuhan.com/" target="_blank">Jiaming Han</a><sup>1*</sup>,</span>
                  <span class="author-block">
                    <a href="https://cs.bit.edu.cn/szdw/jsml/gjjgccrc/lcs_e253eb02bdf246c4a88e1d2499212546/index.htm" target="_blank">Changsheng Li</a><sup>3</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://cs.nju.edu.cn/liyf/index.htm" target="_blank">Yu-Feng Li</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://xyue.io/" target="_blank">Xiangyu Yue</a><sup>1†</sup>
                  </span>

                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>MMLab, The Chinese University of Hong Kong
                      <br><sup>2</sup>National Key Laboratory for Novel Software Technology, Nanjing University
                      <br><sup>3</sup>Beijing Institute of Technology</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup> Equal Contribution <sup></sup></small></span>-->
                  </div>

                  
                  <div class="column has-text-centered">
                    <div class="publication-links">

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.13360" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2410.13360" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Hoar012/RAP-MLLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/Hoar012/RAP-MLLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
  <div class="hero-body">
  <div style="display: flex; justify-content: center; align-items: center;">
  <img width="650px" src="static/images/intro.png" alt="cmp" />
  </div>
  </div>
  <div class="hero-body">
  <h2 class="subtitle has-text-centered">
  <span>Introduce some user-specific concepts to our <b>RAP-LLaVA</b>, it can remember them and achieve excellent performance in a variety of personalized multimodal generation tasks.</span>
  </h2>
  </div>
  </div>
  </section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the <b>R</b>etrieval <b>A</b>ugmented <b>P</b>ersonalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, <i>e.g.</i>, user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method -->
<div class="columns is-centered has-text-centered">
  <HR align="center" style="border:3 double #000000" width="80%" SIZE="5">
  </div>
  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  <h2 class="title is-3">Retrieval-Augmented Personalization</h2>
  <div class="content has-text-justified">
  <p>
Region-of-interest detected by an open world detector are used to retrieve concepts from the database. The images and accompanying information of the retrieved concepts are then integrated into the input for the MLLM.
  </p>
  <img src="static/images/framework.png" alt="MY ALT TEXT" />
  <p>
  Our RAP works in three main steps: Remember, Retrieve and Generate. (a) Remember: RAP includes a designed database to help remember each concept via storing its image and basic information, <i>e.g.</i>, name, avatar and other attributes. (b) Retrieve: When a user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts information are incorporated into the MLLM's input for personalized, knowledge-augmented generation. RAP requires only one image per concept with its basic information for personalization. It allows users to make real-time adjustments to the model's outputs by modifying their personal databases, eliminating the need for retraining.
  </p>
  </div>
  </div>
  </div>
  </div>
  </section>
  <div class="columns is-centered has-text-centered">
  <HR align="center" style="border:3 double #000000" width="80%" SIZE="5">
  </div>

<!-- Caption Demo -->
<div class="columns is-centered has-text-centered">
  </div>
  <section class="section hero">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  <h2 class="title is-3">Examples of Personalized Image Captioning</h2>
  <div class="content has-text-justified">
  <p>
    Image examples of target concepts are shown in the left and captions are shown in the right.
  </p>
  <img src="static/images/captions.png" alt="MY ALT TEXT" />
  <p>
  Our RAP-MLLMs produce clear and accurate captions based on the database content, which also ensures the reliability of the outputs.
  </p>
  </div>
  </div>
  </div>
  </div>
  </section>
  <div class="columns is-centered has-text-centered">
  </div>

<!-- QA Demo -->
<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Examples of Personalized Conversation</h2>
    </div>
  </div>
  <div class="container mt-5">
    <div class="form-row" style="justify-content: center;">
      <div class="form-group col-md-1">
        <div class="col-md-2" style="width: 100%"><label>&nbsp;</label></div>
        <div class="btn-group" role="group" aria-label="Left and Right Controller"
          style="width: 100%;align-items: center;justify-content: center;flex-direction: row;display: flex;">
          <button type="button" class="form-control btn btn-primary" id="prev-question"><i
              class="material-icons">keyboard_arrow_left</i></button>
          <button type="button" class="form-control btn btn-primary" id="next-question"><i
              class="material-icons">keyboard_arrow_right</i></button>
        </div>
      </div>
    </div>

    <!-- Question Card -->
    <div style="display: flex; justify-content: center; align-items: center;">
      <div class="card mb-4" style="width: 60%; display: flex; align-items: center;">
        <div class="card-body" id="selected-question" style="display: flex;">
          <div class="chat-history">
            <!-- Add your chat messages here -->
          </div>

        </div>
      </div>
    </div>

  </div>
</section>

<!-- Recognition Demo -->
<div class="columns is-centered has-text-centered">
</div>
<section class="section hero">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-four-fifths">
<h2 class="title is-3">Examples of Personalized Concept Recognition</h2>
<div class="content has-text-justified">
<img src="static/images/recognition.png" alt="MY ALT TEXT" />
</div>
</div>
</div>
</div>
</section>
<div class="columns is-centered has-text-centered">
</div>

<!-- Concept Edit Demo -->
<div class="columns is-centered has-text-centered">
</div>
<section class="section hero">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-four-fifths">
<h2 class="title is-3">Real-time Concept Editing</h2>
<div class="content has-text-justified">
<p>
  Our models support real-time editing of concepts by modifying the database. Based on the information recorded in the database, our RAP-LLaVA can provide reliable and accurate answers.
</p>
<img src="static/images/edit.png" alt="MY ALT TEXT" />
</div>
</div>
</div>
</div>
</section>
<div class="columns is-centered has-text-centered">
</div>

<!-- Concept Update Demo -->
<div class="columns is-centered has-text-centered">
</div>
<section class="section hero">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-four-fifths">
<h2 class="title is-3">Real-time Concept Updating</h2>
<div class="content has-text-justified">
<p>
  The first caption is generated when toy2 not yet stored in the database. Once the new concept is added, RAP-LLaVA can recognize both toy1 and toy2.
</p>
<img src="static/images/update.png" alt="MY ALT TEXT" />
</div>
</div>
</div>
</div>
</section>
<div class="columns is-centered has-text-centered">
</div>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{hao2024rememberretrievegenerateunderstanding,
        title={Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant}, 
        author={Haoran Hao and Jiaming Han and Changsheng Li and Yu-Feng Li and Xiangyu Yue},
        year={2024},
        eprint={2410.13360},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2410.13360}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

    <script>
      // Handle message showing
      function createChatRow(sender, text, imageSrc) {
        var article = document.createElement("article");
        article.className = "media"
  
        var figure = document.createElement("figure");
        figure.className = "media-left";
  
        var span = document.createElement("span");
        span.className = "icon is-large";
  
        var icon = document.createElement("i");
        // icon.className = "fas fas fa-2x" + (sender === "User" ? " fa-user " : sender === "YoLLaVA" ? " fa-robot" : "");
  
        var media = document.createElement("div");
        media.className = "media-content";
  
        var content = document.createElement("div");
        content.className = "content";
  
        var para = document.createElement("p");
  
        // wrap text in pre tag to preserve whitespace and line breaks
        var pre_text = document.createElement("pre");
        pre_text.style = "background-color: white; font-size: 18px; font-family: Arial; padding: 0; margin: 0; white-space: pre-wrap; word-wrap: break-word;";
        var paraText = document.createTextNode(text);
        pre_text.appendChild(paraText);
  
        var strong = document.createElement("strong");
        strong.innerHTML = sender;
        var br = document.createElement("br");
  
        para.appendChild(strong);
        para.appendChild(br);
        para.appendChild(pre_text);
  
        // Add image if imageSrc is provided
        if (imageSrc) {
          var img = document.createElement("img");
          img.src = imageSrc;
          img.style = "max-width: 95%; max-height: 600px;"; // Adjust the style as needed
          para.appendChild(img);
        }
  
        content.appendChild(para);
        media.appendChild(content);
        span.appendChild(icon);
        figure.appendChild(span);
        if (sender !== "Description") {
          article.appendChild(figure);
        };
        article.appendChild(media);
        return article;
      }
  
      function addMessageToChatHistory(sender, message, imageSrc) {
        const chatHistory = document.querySelector('.chat-history');
        const chatRow = createChatRow(sender, message, imageSrc);
        chatHistory.appendChild(chatRow);
        chatHistory.scrollTop = chatHistory.scrollHeight;
      }
  
      function clearChatHistory() {
        const chatHistory = document.querySelector('.chat-history');
        chatHistory.innerHTML = "";
      }
  
      // 
      const conversations = [
        {
          "description": "QA-example1",
          "turns": [
            ["", "", "./static/images/QA1.png"]
          ]
        },
        {
          "description": "QA-example2",
          "turns": [
            ["", "", "./static/images/QA2.png"]
          ]
        },
        {
          "description": "QA-example3",
          "turns": [
            ["", "", "./static/images/QA3.png"]
          ]
        },
        {
          "description": "QA-example4",
          "turns": [
            ["", "", "./static/images/QA4.png"]
          ]
        },
        {
          "description": "QA-example5",
          "turns": [
            ["", "", "./static/images/QA5.png"]
          ]
        }
      ];
  
      // The current image index
      let currentIndex = 0;
  
      // The function to update the displayed chat history
      function update_dialog_demo() {
        // Clear the chat history
        clearChatHistory();
  
        for (let i = 0; i < conversations[currentIndex].turns.length; i++) {
          if (conversations[currentIndex].turns[i].length == 2) {
            addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1]);
          }
          else {
            addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1], conversations[currentIndex].turns[i][2]);
          }
        }
  
        // scroll to the top of the chat history
        document.querySelector('.chat-history').scrollTop = 0;
      }
  
      // Initialize the displayed image
      update_dialog_demo();
  
      // Event listeners for the buttons
      document.getElementById('prev-question').addEventListener('click', () => {
        currentIndex = (currentIndex - 1 + conversations.length) % conversations.length;
        update_dialog_demo();
      });
  
      document.getElementById('next-question').addEventListener('click', () => {
        currentIndex = (currentIndex + 1) % conversations.length;
        update_dialog_demo();
      });
  
  
    </script>
  </body>
  </html>
